{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "from functools import partial\n",
    "from random import random, seed\n",
    "\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "\n",
    "from Adversary.attacks import *\n",
    "from Adversary.utils import *\n",
    "\n",
    "\n",
    "class Adversary:\n",
    "    def __init__(self, verbose=False, output=None):\n",
    "        self.save_output = partial(pickle_to_file, output=output)\n",
    "        self.print_progress = partial(polite_printer, verbose=verbose)\n",
    "\n",
    "    def generate(self, texts, text_sample_rate=1.0, word_sample_rate=0.3, attacks='all', max_attacks=2, random_seed=None, save=False):\n",
    "        if random_seed:\n",
    "            seed(random_seed)\n",
    "\n",
    "        text_type = type(texts[0])\n",
    "\n",
    "        text_attacks = list(ATTACK_MAP['text'].keys())\n",
    "        word_attacks = list(ATTACK_MAP['word'].keys())\n",
    "        config = self._read_config(attacks)\n",
    "\n",
    "        if text_sample_rate > 1:\n",
    "            num_iters = int(text_sample_rate)\n",
    "        else:\n",
    "            num_iters = 1\n",
    "        total_num = num_iters * len(texts)\n",
    "\n",
    "        # list of tuples containing (attacked text, list of attacks used, index of original text)\n",
    "        generated = []\n",
    "        original = copy(texts)\n",
    "        for iter_no in range(num_iters):\n",
    "            texts = copy(original)\n",
    "            for i in range(len(texts)):\n",
    "                if i % 100 == 0:\n",
    "                    cur_num = iter_no * len(texts) + i\n",
    "                    self.print_progress('Generating attacked version of string {} out of {}'.format(cur_num, total_num))\n",
    "                if text_sample_rate >= random():\n",
    "                    num_attacks = 0\n",
    "                    used_attacks = []\n",
    "                    sorted_attacks = sorted([tuple(c) for c in config.items()],\n",
    "                                            key=lambda a: self._precendence(a[0], word_attacks, text_attacks))\n",
    "                    for attack, pr in sorted_attacks:\n",
    "                        if pr >= random():\n",
    "                            num_attacks += 1\n",
    "                            if num_attacks > max_attacks:\n",
    "                                break\n",
    "                            used_attacks.append(attack)\n",
    "                            if attack in text_attacks:\n",
    "                                texts[i] = ATTACK_MAP['text'][attack](texts[i])\n",
    "                            elif attack in word_attacks:\n",
    "                                blob = TextBlob(texts[i]).tags\n",
    "                                words_attacked = texts[i].split()\n",
    "                                for j, word_with_tag in enumerate(blob):\n",
    "                                    if self._should_attack_word(word_with_tag[1]) and word_sample_rate >= random():\n",
    "                                        try:\n",
    "                                            words_attacked[j] = ATTACK_MAP['word'][attack](word_with_tag[0])\n",
    "                                        except IndexError:\n",
    "                                            pass\n",
    "                                texts[i] = ' '.join(words_attacked)\n",
    "                    generated.append((text_type(texts[i]), used_attacks, i))\n",
    "                else:\n",
    "                    generated.append((text_type(texts[i]), [], i))\n",
    "\n",
    "        if save:\n",
    "            pickle_to_file('generated_text.pkl', generated)\n",
    "\n",
    "        return generated\n",
    "\n",
    "    def _read_config(self, attacks):\n",
    "        if attacks == 'all':\n",
    "            config = [(t_a, 1. / len(ATTACK_MAP['text'])) for t_a in ATTACK_MAP['text']] + \\\n",
    "                [(w_a, 1. / len(ATTACK_MAP['word'])) for w_a in ATTACK_MAP['word']]\n",
    "            return dict(config)\n",
    "        elif isinstance(attacks, list):\n",
    "            selected_text_attacks = [t_a for t_a in attacks if t_a in ATTACK_MAP['text']]\n",
    "            selected_word_attacks = [w_a for w_a in attacks if w_a in ATTACK_MAP['word']]\n",
    "            config = [(t_a, 1. / len(selected_text_attacks)) for t_a in selected_text_attacks] + \\\n",
    "                [(w_a, 1. / len(selected_word_attacks)) for w_a in selected_word_attacks]\n",
    "            return dict(config)\n",
    "        elif isinstance(attacks, dict):\n",
    "            return attacks\n",
    "\n",
    "    def _precendence(self, attack, word_attacks, text_attacks):\n",
    "        if attack in word_attacks:\n",
    "            return 2\n",
    "        elif attack == 'good_word_attack':\n",
    "            return 1\n",
    "        elif attack in text_attacks:\n",
    "            return 0\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _should_attack_word(self, tag):\n",
    "        return tag[0] in ['N', 'V', 'J'] or tag == 'CD'\n",
    "\n",
    "    def attack(self, texts_original, texts_generated, predict_function, save=False):\n",
    "        \n",
    "        texts_original_flat = [texts_original[t_g[2]] for t_g in texts_generated]\n",
    "        texts_generated_flat = [t_g[0] for t_g in texts_generated]\n",
    "        attacks_applied = [t_g[1] for t_g in texts_generated]\n",
    "\n",
    "        original_preds = [predict_function(t_o) for t_o in texts_original_flat]\n",
    "        generated_preds = [predict_function(t_g) for t_g in texts_generated_flat]\n",
    "\n",
    "        self.print_progress('Accuracy on original texts: {}'.format(1. * sum(original_preds) / len(original_preds)))\n",
    "        self.print_progress('Accuracy on generated texts: {}'.format(1. * sum(generated_preds) / len(generated_preds)))\n",
    "\n",
    "        misclassifications_single = self._get_misclassifications_single(original_preds, generated_preds, attacks_applied)\n",
    "        misclassifications_group = self._get_misclassifications_group(original_preds, generated_preds, attacks_applied)\n",
    "\n",
    "        if save:\n",
    "            pickle_to_file('misclassifications_df_single.pkl', misclassifications_single)\n",
    "            pickle_to_file('misclassifications_df_group.pkl', misclassifications_group)\n",
    "\n",
    "        return misclassifications_single, misclassifications_group\n",
    "\n",
    "    def _get_misclassifications_single(self, original_preds, generated_preds, attacks_applied):\n",
    "        results = zip(original_preds, generated_preds, attacks_applied)\n",
    "        misclassifications_single = dict([(att, 0) for att in flatten_unique(attacks_applied)])\n",
    "        always_wrong_single = copy(misclassifications_single)\n",
    "        never_wrong_single = copy(misclassifications_single)\n",
    "\n",
    "        for i, res in enumerate(results):\n",
    "            original, generated, applied = res[0], res[1], res[2]\n",
    "            if original == 1 and generated == 0:\n",
    "                for attack in applied:\n",
    "                    misclassifications_single[attack] += 1\n",
    "            elif original == 1 and generated == 1:\n",
    "                for attack in applied:\n",
    "                    always_wrong_single[attack] += 1\n",
    "            elif original == 0 and generated == 0:\n",
    "                for attack in applied:\n",
    "                    never_wrong_single[attack] += 1\n",
    "\n",
    "        index_fancy = fancy_titles(misclassifications_single.keys())\n",
    "        return pd.DataFrame(\n",
    "            list(zip(misclassifications_single.values(), always_wrong_single.values(), never_wrong_single.values())),\n",
    "            index=index_fancy,\n",
    "            columns=['Caused Misclassifications', 'Always Misclassified', 'Never Misclassified']\n",
    "        ).rename_axis('Attack')\n",
    "\n",
    "    def _get_misclassifications_group(self, original_preds, generated_preds, attacks_applied):\n",
    "        results = list(zip(original_preds, generated_preds, attacks_applied))\n",
    "        attacks_used = flatten_unique(attacks_applied)\n",
    "        misclassifications_group = dict([(','.join(sorted(att)), 0) for att in attacks_applied])\n",
    "        always_wrong_group = copy(misclassifications_group)\n",
    "        never_wrong_group = copy(misclassifications_group)\n",
    "        for i, res in enumerate(results):\n",
    "            original, generated, applied = res[0], res[1], res[2]\n",
    "            if original == 1 and generated == 0:\n",
    "                misclassifications_group[','.join(sorted(applied))] += 1\n",
    "            elif original == 1 and generated == 1:\n",
    "                always_wrong_group[','.join(sorted(applied))] += 1\n",
    "            elif original == 0 and generated == 0:\n",
    "                never_wrong_group[','.join(sorted(applied))] += 1\n",
    "\n",
    "        cols = sorted(attacks_used) + ['Caused Misclassifications', 'Always Misclassified', 'Never Misclassified']\n",
    "        all_combos = combinations_of_len(attacks_used, max([len(res[2]) for res in results]))\n",
    "        misclassifications_df_list = []\n",
    "        for i, combo in enumerate(all_combos):\n",
    "            row = [' '] * len(cols)\n",
    "            for att in combo:\n",
    "                row[cols.index(att)] = 'X'\n",
    "            row[-3] = misclassifications_group.get(','.join(sorted(combo)), 0)\n",
    "            row[-2] = never_wrong_group.get(','.join(sorted(combo)), 0)\n",
    "            row[-1] = always_wrong_group.get(','.join(sorted(combo)), 0)\n",
    "            misclassifications_df_list.append(row)\n",
    "        misclassifications_df_list = sorted(misclassifications_df_list, key=lambda x: x[-3], reverse=True)\n",
    "\n",
    "        cols_fancy = fancy_titles(cols)\n",
    "        df = pd.DataFrame(misclassifications_df_list, columns=cols_fancy)\n",
    "        return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
